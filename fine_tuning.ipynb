{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzbQj-kubcx1",
        "outputId": "de4f93e7-6c1e-4169-ad28-4f99046422ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (23.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (3.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install sentencepiece datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2FOIz3BcBn4",
        "outputId": "1dc23da4-1337-439c-d310-746c5a1e1589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Mar  9 05:56:22 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    21W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKt0Bn1sbRb9",
        "outputId": "0ce280e5-29ce-4b21-e600-3865b8a42fc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "import pandas as pd\n",
        "from scipy.special import softmax\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEUW6MftbRb-"
      },
      "outputs": [],
      "source": [
        "# import an unlabelled dataset and a validation dataset\n",
        "train = pd.read_csv('/content/drive/MyDrive/val/df_sentiment_train_text.csv',header=None).values[:,0].tolist()\n",
        "val = pd.read_csv('/content/drive/MyDrive/val/df_sentiment_val_text.csv',header=None).values[:,0].tolist()\n",
        "val_label = pd.read_csv('/content/drive/MyDrive/val/df_sentiment_val_label.csv',header=None).values[:,0].tolist()\n",
        "val_label = [2 if i == 'positive' else i for i in val_label]\n",
        "val_label = [1 if i == 'neutral' else i for i in val_label]\n",
        "val_label = [0 if i == 'negative' else i for i in val_label]\n",
        "train = [str(x) for x in train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUE9Nv9DbRb-"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels).to('cuda') if torch.cuda.is_available() else torch.tensor(labels)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# subclassing the Trainer class and overriding the compute_loss function with a custom loss function to balance the classes\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, class_weights, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\").to('cuda')\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # compute custom loss (suppose one has 3 labels with different weights)\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# self training class\n",
        "class SelfTraining:\n",
        "    # first initialize the class with the model and tokenizer\n",
        "    def __init__(self,model,tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.CUDA = True\n",
        "        self.LR = 2e-5\n",
        "        self.EPOCHS = 5\n",
        "        self.BATCH_SIZE =100\n",
        "        self.MAX_TRAINING_EXAMPLES = -1\n",
        "        self.imbalanced = True\n",
        "        self.best_acc = 0\n",
        "        self.acc = []\n",
        "        self.class_weights = torch.tensor([1,2,10],dtype=torch.float32).to('cuda') if torch.cuda.is_available() else torch.tensor(class_weights,dtype=torch.float32)\n",
        "\n",
        "    # load the unlabeled training data and validation data with labels\n",
        "    def load_data(self,train,val,val_label):\n",
        "        self.train_data = train\n",
        "        self.val_data = val\n",
        "        self.val_data_label = val_label\n",
        "\n",
        "    # preprocess the data\n",
        "    def preprocess(self,corpus):\n",
        "        outcorpus = []\n",
        "        for text in corpus:\n",
        "            new_text = []\n",
        "            for t in text.split(\" \"):\n",
        "                t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "                t = 'http' if t.startswith('http') else t\n",
        "                new_text.append(t)\n",
        "            new_text = \" \".join(new_text)\n",
        "            outcorpus.append(new_text)\n",
        "        return outcorpus\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self,text, cuda=True):\n",
        "        text = self.preprocess(text)\n",
        "        encoded_input = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "        if cuda:\n",
        "            encoded_input.to('cuda')\n",
        "            output = self.model(**encoded_input)\n",
        "            scores = output[0].detach().cpu().numpy()\n",
        "        else:\n",
        "            output = self.model(**encoded_input)\n",
        "            scores = output[0].detach().numpy()\n",
        "\n",
        "        scores = softmax(scores, axis=-1)\n",
        "        return scores\n",
        "\n",
        "    # predict the labels and construct MyDataset for training data\n",
        "    def predict(self):\n",
        "        if self.CUDA:\n",
        "            self.model.to('cuda')\n",
        "        corpus = self.train_data\n",
        "        dl = DataLoader(corpus, batch_size=self.BATCH_SIZE)\n",
        "        all_preds = []\n",
        "        all_scores = []\n",
        "\n",
        "        for idx,batch in enumerate(dl):\n",
        "            text = self.preprocess(batch)\n",
        "            scores = self.forward(text, cuda=self.CUDA)\n",
        "            preds = np.argmax(scores, axis=-1)\n",
        "            all_preds.extend(preds)\n",
        "            all_scores.extend(np.max(scores, axis=-1))\n",
        "\n",
        "        print('done')\n",
        "        train = pd.DataFrame(corpus)\n",
        "        train['labels'] = all_preds\n",
        "        train['scores'] = all_scores\n",
        "        train.columns = ['texts','labels','scores']\n",
        "        train = train[train['scores'] > 0.9].reset_index()\n",
        "        if len(train) == 0:\n",
        "            print(\"No new training examples found\")\n",
        "            return False\n",
        "\n",
        "        else:\n",
        "            train_label = train['labels'].values.tolist()\n",
        "            train_label = [2 if i == 'positive' else i for i in train_label]\n",
        "            train_label = [1 if i == 'neutral' else i for i in train_label]\n",
        "            train_label = [0 if i == 'negative' else i for i in train_label]\n",
        "            if self.imbalanced:\n",
        "                class_counts = np.bincount(train_label)\n",
        "                class_weights = [sum(class_counts) / count for count in class_counts]\n",
        "                self.class_weights = torch.tensor(class_weights,dtype=torch.float32).to('cuda') if torch.cuda.is_available() else torch.tensor(class_weights,dtype=torch.float32)\n",
        "            train_text = train['texts'].values.tolist()\n",
        "            train_encodings = self.tokenizer(train_text, return_tensors='pt', padding=True, truncation=True)\n",
        "            if self.CUDA:\n",
        "                train_encodings.to('cuda')\n",
        "            train_dataset = MyDataset(train_encodings, train_label)\n",
        "            self.train_dataset = train_dataset\n",
        "            print(\"New training examples found: \",len(train_dataset))\n",
        "            return True\n",
        "\n",
        "    # encode the validation data and construct MyDataset for it\n",
        "    def encode_val(self):\n",
        "        val_encodings = self.tokenizer(self.val_data, return_tensors='pt', padding=True, truncation=True)\n",
        "        if self.CUDA:\n",
        "            val_encodings.to('cuda')\n",
        "        val_dataset = MyDataset(val_encodings, self.val_data_label)\n",
        "        self.val_dataset = val_dataset\n",
        "\n",
        "    # train the model iteratively with a parameter for number of iterations\n",
        "    def train(self,iterations=2):\n",
        "        if self.CUDA:\n",
        "            self.model.to('cuda')\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in self.model.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "        self.encode_val()\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='/content/drive/MyDrive/results',                   # output directory\n",
        "            num_train_epochs=self.EPOCHS,                  # total number of training epochs\n",
        "            per_device_train_batch_size=self.BATCH_SIZE,   # batch size per device during training\n",
        "            per_device_eval_batch_size=self.BATCH_SIZE,    # batch size for evaluation\n",
        "            warmup_steps=0,                         # number of warmup steps for learning rate scheduler\n",
        "            weight_decay=0.01,                        # strength of weight decay\n",
        "            logging_dir='/content/drive/MyDrive/results/logs',                     # directory for storing logs\n",
        "            logging_steps=10,                         # when to print log\n",
        "            load_best_model_at_end=True,              # load or not best model at the end\n",
        "            evaluation_strategy='steps',              # when to evaluate\n",
        "            dataloader_pin_memory=False\n",
        "        )\n",
        "        for i in range(iterations):\n",
        "            print(\"Iteration: \",i+1)\n",
        "            if self.predict():\n",
        "                print(len(self.train_dataset))\n",
        "                trainer = CustomTrainer(\n",
        "                    model=self.model,                              # the instantiated ðŸ¤— Transformers model to be trained\n",
        "                    args=training_args,                       # training arguments, defined above\n",
        "                    train_dataset=self.train_dataset,              # training dataset\n",
        "                    eval_dataset=self.val_dataset,                # evaluation dataset\n",
        "                    class_weights=self.class_weights\n",
        "                )\n",
        "                trainer.train()\n",
        "\n",
        "                # evaluate on the validation set\n",
        "                test_preds_raw, test_labels , _ = trainer.predict(self.val_dataset)\n",
        "                test_preds = np.argmax(test_preds_raw, axis=-1)\n",
        "                print(classification_report(test_labels, test_preds, digits=3))\n",
        "\n",
        "                # save the accuracy of best model of the iteration for comparison later\n",
        "                self.acc.append(accuracy_score(test_labels, test_preds))\n",
        "\n",
        "                # save best model of the iteration if it is the best model so far\n",
        "                print(\"Accuracy: \",str(self.acc[-1]),\" - iteration: \",str(i+1))\n",
        "                if self.acc[-1] > self.best_acc:\n",
        "                    self.best_acc = self.acc[-1]\n",
        "                    trainer.save_model('/content/drive/MyDrive/results/iteration_'+str(i+1))\n",
        "                    print(\"Model saved! - iteration: \",str(i+1))\n",
        "                if i + 1 == iterations:\n",
        "                    trainer.save_model('/content/drive/MyDrive/results/iteration_'+str(i+1))\n",
        "                    print(\"Model saved! - iteration: \",str(i+1))\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    def full_train(self,iterations=2):\n",
        "      data = pd.read_csv('/content/drive/MyDrive/val/df_sentiment_test_text.csv',header=None).values[:,0].tolist()\n",
        "      data_label = pd.read_csv('/content/drive/MyDrive/val/df_sentiment_test_label.csv',header=None).values[:,0].tolist()\n",
        "      data_encodings = test.tokenizer(data, return_tensors='pt', padding=True, truncation=True)\n",
        "      data_dataset = MyDataset(data_encodings, data_label)\n",
        "      test.train_dataset = data_dataset\n",
        "      if self.CUDA:\n",
        "          self.model.to('cuda')\n",
        "          data_encodings.to('cuda')\n",
        "      for param in self.model.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "      self.encode_val()\n",
        "      training_args = TrainingArguments(\n",
        "          output_dir='/content/drive/MyDrive/results',                   # output directory\n",
        "          num_train_epochs=self.EPOCHS,                  # total number of training epochs\n",
        "          per_device_train_batch_size=self.BATCH_SIZE,   # batch size per device during training\n",
        "          per_device_eval_batch_size=self.BATCH_SIZE,    # batch size for evaluation\n",
        "          warmup_steps=0,                         # number of warmup steps for learning rate scheduler\n",
        "          weight_decay=0.01,                        # strength of weight decay\n",
        "          logging_dir='/content/drive/MyDrive/results/logs',                     # directory for storing logs\n",
        "          logging_steps=10,                         # when to print log\n",
        "          load_best_model_at_end=True,              # load or not best model at the end\n",
        "          evaluation_strategy='steps',              # when to evaluate\n",
        "          dataloader_pin_memory=False\n",
        "      )\n",
        "      for i in range(iterations):\n",
        "          print(\"Iteration: \",i+1)\n",
        "          print(len(self.train_dataset))\n",
        "          trainer = CustomTrainer(\n",
        "              model=self.model,                              # the instantiated ðŸ¤— Transformers model to be trained\n",
        "              args=training_args,                       # training arguments, defined above\n",
        "              train_dataset=self.train_dataset,              # training dataset\n",
        "              eval_dataset=self.val_dataset,                # evaluation dataset\n",
        "              class_weights=self.class_weights\n",
        "          )\n",
        "          trainer.train()\n",
        "\n",
        "          # evaluate on the validation set\n",
        "          test_preds_raw, test_labels , _ = trainer.predict(self.val_dataset)\n",
        "          test_preds = np.argmax(test_preds_raw, axis=-1)\n",
        "          print(classification_report(test_labels, test_preds, digits=3))\n",
        "\n",
        "          # save the accuracy of best model of the iteration for comparison later\n",
        "          self.acc.append(accuracy_score(test_labels, test_preds))\n",
        "\n",
        "          # save best model of the iteration if it is the best model so far\n",
        "          print(\"Accuracy: \",str(self.acc[-1]),\" - iteration: \",str(i+1))\n",
        "          if self.acc[-1] > self.best_acc:\n",
        "              self.best_acc = self.acc[-1]\n",
        "              trainer.save_model('/content/drive/MyDrive/results/iteration_'+str(i+1))\n",
        "              print(\"Model saved! - iteration: \",str(i+1))\n",
        "          if i + 1 == iterations:\n",
        "              trainer.save_model('/content/drive/MyDrive/results/iteration_'+str(i+1))\n",
        "              print(\"Model saved! - iteration: \",str(i+1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3PtCStpbRcA"
      },
      "outputs": [],
      "source": [
        "MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\" # use this to finetune the sentiment classifier\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/results/base',num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixo9zb-sbRcA"
      },
      "outputs": [],
      "source": [
        "test = SelfTraining(model,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGJTNksYbRcA"
      },
      "outputs": [],
      "source": [
        "test.load_data(train,val,val_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7TO4lQgbRcB"
      },
      "outputs": [],
      "source": [
        "test.full_train(iterations=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VWUA_MLddBL"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a8aac0b6a3e67f35bcd67088a857342b2a6b50e1135570ee0f841fb10732c056"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
